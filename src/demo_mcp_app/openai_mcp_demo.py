"""
Optimized OpenAI Native MCP Client

This module provides an optimized, production-ready implementation of the OpenAI + MCP integration
with proper resource management, error handling, and configurability.
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import Optional, List, Dict, Any

# Try to import optional dependencies
try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False
    def load_dotenv():
        pass

# Try to import agents module - gracefully handle if not available
try:
    from agents import Agent, Runner
    from agents.mcp import MCPServerStdio, MCPServerStdioParams
    AGENTS_AVAILABLE = True
except ImportError:
    # Create mock classes for testing when agents module is not available
    AGENTS_AVAILABLE = False
    
    class Agent:
        def __init__(self, name: str, model: str = "gpt-4o-mini", instructions: str = ""):
            self.name = name
            self.model = model
            self.instructions = instructions
    
    class Runner:
        def __init__(self, agent: Agent, mcp_servers: List = None):
            self.agent = agent
            self.mcp_servers = mcp_servers or []
        
        async def run_sync(self, message: str) -> str:
            return f"Mock response for: {message}"
    
    class MCPServerStdio:
        def __init__(self, params):
            self.params = params
    
    class MCPServerStdioParams:
        def __init__(self, command: str, args: List[str], env: Optional[Dict] = None):
            self.command = command
            self.args = args
            self.env = env or {}

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class MCPConfig:
    """Configuration for MCP server connection."""
    command: str = "docker"
    args: List[str] = None
    container_name: str = "mcp-server-atlassian"
    mcp_command: str = "mcp-atlassian"
    
    def __post_init__(self):
        import os
        
        # Allow environment variable override if needed
        self.container_name = os.getenv('MCP_CONTAINER_NAME', self.container_name)
            
        if self.args is None:
            self.args = ["exec", "-i", self.container_name, self.mcp_command]

@dataclass 
class AgentConfig:
    """Configuration for OpenAI Agent."""
    name: str = "jira-assistant"
    model: str = "gpt-4o-mini"
    instructions: str = """You are a helpful Jira assistant. You have access to Jira data through MCP tools.

When users ask about projects, issues, or want to manage Jira data, use the available MCP tools to get real information.
Always provide clear, conversational responses with the actual data you retrieve.

IMPORTANT ERROR HANDLING GUIDELINES:
- If creating issue links fails due to link type errors, try common alternatives: "Blocks", "Relates", "Depends on"
- If an operation fails, explain what happened and suggest alternatives
- Always check available link types first if you encounter link creation errors
- Be resilient and try multiple approaches when encountering API errors"""

class OptimizedMCPClient:
    """
    Optimized MCP client with proper resource management, error handling, and conversation context.
    
    Features:
    - Connection pooling and reuse
    - Proper async context management
    - Comprehensive error handling
    - Configurable parameters
    - Structured logging
    - Conversation context via OpenAI Responses API
    """
    
    def __init__(self, mcp_config: MCPConfig = None, agent_config: AgentConfig = None):
        self.mcp_config = mcp_config or MCPConfig()
        self.agent_config = agent_config or AgentConfig()
        self._server: Optional[MCPServerStdio] = None
        self._agent: Optional[Agent] = None
        self._connected = False
        self._conversation_history: List[Dict[str, str]] = []
        self._last_response_id: Optional[str] = None
        
    @asynccontextmanager
    async def connect(self):
        """Async context manager for MCP connection with automatic cleanup."""
        try:
            await self._establish_connection()
            yield self
        finally:
            await self._cleanup_connection()
    
    async def _establish_connection(self):
        """Establish MCP server connection and create agent."""
        if self._connected:
            logger.warning("‚ö†Ô∏è  Already connected to MCP server")
            return
            
        try:
            logger.info("üîå Connecting to MCP server...")
            
            if AGENTS_AVAILABLE:
                # Create server parameters
                server_params = MCPServerStdioParams()
                server_params.update({
                    "command": self.mcp_config.command,
                    "args": self.mcp_config.args
                })
                
                # Create and connect to server
                self._server = MCPServerStdio(server_params)
                await self._server.connect()
                logger.info("üì° MCP server connection established")
                
                # Create agent
                logger.info("ü§ñ Creating OpenAI agent...")
                self._agent = Agent(
                    name=self.agent_config.name,
                    model=self.agent_config.model,
                    instructions=self.agent_config.instructions,
                    mcp_servers=[self._server]
                )
            else:
                # Use mock implementations for testing
                logger.info("üß™ Using mock implementations (agents module not available)")
                server_params = MCPServerStdioParams(
                    command=self.mcp_config.command,
                    args=self.mcp_config.args
                )
                self._server = MCPServerStdio(server_params)
                self._agent = Agent(
                    name=self.agent_config.name,
                    model=self.agent_config.model,
                    instructions=self.agent_config.instructions
                )
            
            self._connected = True
            logger.info("‚úÖ MCP server connected and agent created successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to establish MCP connection: {e}")
            await self._cleanup_connection()
            raise
    
    async def _cleanup_connection(self):
        """Clean up MCP server connection and reset conversation state."""
        if self._server and self._connected:
            try:
                logger.info("üßπ Cleaning up MCP server connection...")
                await self._server.cleanup()
                logger.info("‚úÖ MCP server connection closed gracefully")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Warning during server cleanup: {e}")
            finally:
                self._server = None
                self._agent = None
                self._connected = False
                self._conversation_history = []
                self._last_response_id = None
                logger.info("üîÑ Connection state reset")
    
    async def query(self, question: str, max_retries: int = 3, use_conversation_context: bool = True) -> str:
        """
        Execute a natural language query against the MCP server with conversation context.
        
        Args:
            question: Natural language question
            max_retries: Maximum number of retry attempts
            use_conversation_context: Whether to use previous conversation context
            
        Returns:
            Response string from the agent
            
        Raises:
            RuntimeError: If not connected or query fails
        """
        if not self._connected or not self._agent:
            raise RuntimeError("Client not connected. Use async with client.connect():")
        
        for attempt in range(max_retries):
            try:
                # Use conversation context if enabled and available
                previous_response_id = self._last_response_id if use_conversation_context else None
                if previous_response_id:
                    logger.info(f"üß† Using conversation context: {previous_response_id[:12]}...")
                else:
                    logger.info("üÜï Starting fresh conversation (no previous context)")
                
                logger.info(f"‚ö° Executing query (attempt {attempt + 1}/{max_retries})")
                
                if AGENTS_AVAILABLE:
                    result = await Runner.run(
                        self._agent, 
                        question,
                        previous_response_id=previous_response_id
                    )
                    
                    response = result.final_output_as(str)
                    response_id = result.last_response_id
                else:
                    # Mock implementation for testing
                    import uuid
                    response = f"Mock response for: {question}"
                    response_id = str(uuid.uuid4())
                
                # Update conversation state
                self._conversation_history.append({
                    "question": question,
                    "response": response,
                    "response_id": response_id
                })
                self._last_response_id = response_id
                
                logger.info(f"‚úÖ Query successful! New response ID: {response_id[:12]}...")
                return response
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Query attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå Query failed after {max_retries} attempts")
                    raise
                await asyncio.sleep(1)  # Brief delay before retry
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get the conversation history."""
        return self._conversation_history.copy()
    
    def clear_conversation_context(self):
        """Clear conversation context while keeping connection."""
        self._conversation_history = []
        self._last_response_id = None
        logger.info("üß† Conversation context cleared")
    
    def get_last_response_id(self) -> Optional[str]:
        """Get the last response ID for manual context management."""
        return self._last_response_id
    
    async def batch_query(self, questions: List[str], use_conversation_context: bool = True) -> Dict[str, str]:
        """
        Execute multiple queries efficiently with conversation context.
        
        Args:
            questions: List of questions to ask
            use_conversation_context: Whether to maintain context between questions
            
        Returns:
            Dictionary mapping questions to responses
        """
        results = {}
        logger.info(f"üîÑ Starting batch query: {len(questions)} questions")
        
        for i, question in enumerate(questions, 1):
            try:
                logger.info(f"üìù Processing question {i}/{len(questions)}")
                response = await self.query(question, use_conversation_context=use_conversation_context)
                results[question] = response
                logger.info(f"‚úÖ Question {i} completed successfully")
            except Exception as e:
                logger.error(f"‚ùå Failed to process question {i}: {e}")
                results[question] = f"Error: {e}"
        
        logger.info(f"üèÅ Batch query completed: {len(results)} results")
        return results

def _format_issue_list(response: str) -> str:
    """Format issue list responses with better visual hierarchy."""
    import re
    
    # Look for numbered issues in the response
    lines = []
    
    # Split response into lines and process
    response_lines = response.split('\n')
    current_issue = []
    
    for line in response_lines:
        line = line.strip()
        if not line:
            continue
            
        # Detect issue patterns like "1. **[MCP-382]**" or "**[MCP-382]**"
        if re.search(r'\*\*\[?MCP-\d+\]?\*\*', line) and ('Implement' in line or 'Setup' in line or 'Build' in line):
            if current_issue:
                lines.extend(_format_single_issue(current_issue))
                lines.append("")  # Add spacing between issues
                current_issue = []
            current_issue.append(line)
        elif current_issue and line:
            current_issue.append(line)
        elif not current_issue and line:
            # Header text
            lines.append(line)
            if "MCP" in line and "project" in line:
                lines.append("")
    
    # Format the last issue
    if current_issue:
        lines.extend(_format_single_issue(current_issue))
    
    return '\n'.join(lines)

def _format_single_issue(issue_lines: List[str]) -> List[str]:
    """Format a single issue with improved spacing and visual hierarchy."""
    if not issue_lines:
        return []
    
    formatted = []
    full_text = ' '.join(issue_lines)
    
    # Extract issue key and title using regex
    import re
    issue_match = re.search(r'MCP-(\d+)', full_text)
    
    if issue_match:
        issue_key = f"MCP-{issue_match.group(1)}"
        
        # Extract title - look for pattern after the issue key
        title_patterns = [
            rf'\[?{re.escape(issue_key)}\]?\*\*:?\s*-?\s*([^-\n]+?)(?:\s*-|\s*\*\*|\s*$)',
            rf'\*\*\[?{re.escape(issue_key)}\]?\*\*:?\s*-?\s*([^-\n]+?)(?:\s*-|\s*Description|\s*Status|$)'
        ]
        
        title = "Unknown Title"
        for pattern in title_patterns:
            title_match = re.search(pattern, full_text)
            if title_match:
                title = title_match.group(1).strip()
                break
        
        formatted.append(f"üéØ {issue_key}: {title}")
        
        # Extract description if available
        desc_match = re.search(r'Description[:\*\s]*([^-\n]+?)(?:\s*-|\s*Status|$)', full_text)
        if desc_match:
            desc = desc_match.group(1).strip()
            if desc and len(desc) > 10:  # Only show substantial descriptions
                formatted.append(f"   üìù {desc}")
        
        # Extract status, priority, etc.
        status_match = re.search(r'Status[:\*\s]*([^-\n]+?)(?:\s*-|\s*Priority|$)', full_text)
        if status_match:
            status = status_match.group(1).strip().replace('*', '')
            if status:
                formatted.append(f"   üìä Status: {status}")
        
        priority_match = re.search(r'Priority[:\*\s]*([^-\n]+?)(?:\s*-|\s*Assignee|$)', full_text)
        if priority_match:
            priority = priority_match.group(1).strip().replace('*', '')
            if priority:
                formatted.append(f"   ‚ö° Priority: {priority}")
    
    return formatted if formatted else [f"üéØ {' '.join(issue_lines)[:100]}..."]

def _format_dependency_suggestions(response: str) -> str:
    """Format dependency suggestion responses with clear hierarchy and pairings."""
    import re
    
    lines = []
    
    # Look for dependency patterns in the response
    for line in response.split('\n'):
        line = line.strip()
        if not line:
            continue
        
        # Handle header lines
        if line.startswith("Based on") or "here's a suggested" in line.lower():
            lines.append(f"üéØ {line}")
            lines.append("")
        elif line.startswith("###") or line.startswith("**") and line.endswith("**"):
            clean_line = line.replace("#", "").replace("*", "").strip()
            lines.append(f"üìã {clean_line}")
            lines.append("")
        # Look for dependency relationship patterns
        elif re.search(r'MCP-\d+.*(?:should|must|needs).*(?:before|after|depend).*MCP-\d+', line, re.IGNORECASE):
            # This is a dependency relationship description
            lines.append(f"üîó {line}")
        elif re.search(r'MCP-\d+.*(?:‚Üí|->).*MCP-\d+', line):
            # Arrow-based dependency
            lines.append(f"üîó {line}")
        elif re.search(r'^\d+\.\s.*MCP-\d+.*MCP-\d+', line):
            # Numbered dependency item with two issue keys
            clean_line = re.sub(r'^\d+\.\s*', '', line)
            lines.append(f"üîó {clean_line}")
        elif re.search(r'MCP-\d+', line) and len(line) > 20:
            # Line contains issue keys and substantial content
            if "depends on" in line.lower() or "blocks" in line.lower() or "requires" in line.lower():
                lines.append(f"   üìå {line}")
            else:
                lines.append(f"   üí° {line}")
        else:
            # Regular content
            if len(line) > 5:
                lines.append(f"   {line}")
    
    return '\n'.join(lines)

def _format_link_confirmation(response: str) -> str:
    """Format link creation confirmation with clear success indicators and specific issue details."""
    import re
    
    lines = []
    
    # Check for successful link creation
    if any(word in response.lower() for word in ['successfully', 'created', 'linked', 'established']):
        lines.append("‚úÖ DEPENDENCY LINK CREATED SUCCESSFULLY")
        lines.append("")
        
        # Extract specific issue keys from the response
        issue_keys = re.findall(r'MCP-\d+', response)
        if len(issue_keys) >= 2:
            lines.append(f"üîó Linked Issues: {issue_keys[0]} ‚Üê depends on ‚Üí {issue_keys[1]}")
            lines.append("")
        
        # Look for link type information
        link_type_match = re.search(r'(?:link type|type)[:"\s]*([A-Za-z\s]+?)(?:["\n]|$)', response, re.IGNORECASE)
        if link_type_match:
            link_type = link_type_match.group(1).strip()
            lines.append(f"ÔøΩ Link Type: {link_type}")
            lines.append("")
        
        # Extract any additional details from the response
        for line in response.split('\n'):
            line = line.strip()
            if line and not any(skip in line.lower() for skip in ['successfully', 'created', 'link has been']):
                if 'MCP-' in line and len(line) > 10:
                    lines.append(f"üìå {line}")
        
        if not any('üìå' in line for line in lines):
            lines.append("üéØ This creates a dependency relationship where one issue")
            lines.append("   must be completed before work can proceed on the other.")
    else:
        # Handle error cases or other responses
        lines.append("üìã LINK CREATION RESPONSE")
        lines.append("")
        
        # Look for error information
        if 'error' in response.lower() or 'failed' in response.lower():
            lines.append("‚ùå Issue encountered:")
            lines.append("")
        
        # Format the actual response content
        for line in response.split('\n'):
            line = line.strip()
            if line:
                lines.append(f"   {line}")
    
    return '\n'.join(lines)

def _format_default_response(response: str) -> str:
    """Default formatting with improved readability and spacing."""
    import textwrap
    
    # Simple paragraph formatting with better spacing
    paragraphs = response.split('\n\n')
    formatted_paragraphs = []
    
    for para in paragraphs:
        para = para.strip()
        if para:
            # Wrap long paragraphs
            wrapped = textwrap.fill(para, width=75)
            formatted_paragraphs.append(wrapped)
    
    return '\n\n'.join(formatted_paragraphs)

async def demo_optimized_client():
    """Demonstrate the optimized MCP client with enhanced CLI output."""
    # Load environment variables
    load_dotenv()
    
    # Configure client
    mcp_config = MCPConfig()
    agent_config = AgentConfig()
    
    client = OptimizedMCPClient(mcp_config, agent_config)
    
    # Example queries with improved error handling guidance
    questions = [
        #"What Jira projects do I have access to?",
        "Show me issues in the MCP project", 
        "Based on the summary and description fields of each work item in the MCP project, suggest where dependencies should be set using linked issues",
        "First check what issue link types are available in this Jira instance, then set the first dependency from your previous suggestions using an appropriate link type"
    ]
    
    try:
        # Print header
        print("\n" + "=" * 80)
        print("üöÄ OpenAI + MCP Integration Demo")
        print("   Intelligent Jira Assistant with Conversation Memory")
        print("=" * 80)
        
        # Show configuration
        print(f"\nüîß Configuration:")
        print(f"   ‚Ä¢ Container: {mcp_config.container_name}")
        print(f"   ‚Ä¢ Agent Model: {agent_config.model}")
        print(f"   ‚Ä¢ Agent Name: {agent_config.name}")
        print(f"   ‚Ä¢ Total Questions: {len(questions)}")
        
        # Use the client with proper connection management
        async with client.connect():
            print(f"\n‚úÖ Connected to MCP server successfully!")
            
            # Option 1: Individual queries with conversation context
            for i, question in enumerate(questions, 1):
                print(f"\n" + "‚îÄ" * 80)
                print(f"üéØ Question {i} of {len(questions)}")
                print("‚îÄ" * 80)
                print(f"‚ùì {question}")
                print()
                
                # Show conversation context info with better formatting
                context_info = ""
                if client._last_response_id:
                    context_info = f"ÔøΩ Context: Using memory from response {client._last_response_id[:12]}..."
                else:
                    context_info = "ÔøΩ Context: Starting fresh conversation (no previous context)"
                
                print(f"{context_info}")
                print(f"üìä History: {len(client.get_conversation_history())} previous messages")
                print()
                
                try:
                    print("‚è≥ Processing query...")
                    response = await client.query(question)
                    
                    # Format the response with enhanced visual appeal
                    print("\n" + "‚ïê" * 80)
                    print("‚ú® RESPONSE")
                    print("‚ïê" * 80)
                    print()
                    
                    # Parse and format different types of content
                    if "Here are some" in response and "issues" in response:
                        # Format issue list with better spacing
                        formatted_response = _format_issue_list(response)
                    elif "dependencies" in response.lower() and "suggest" in response.lower():
                        # Format dependency suggestions with better spacing
                        formatted_response = _format_dependency_suggestions(response)
                    elif "link has been" in response and "created" in response:
                        # Format link creation confirmation with better spacing
                        formatted_response = _format_link_confirmation(response)
                    else:
                        # Default formatting with improved spacing
                        formatted_response = _format_default_response(response)
                    
                    print(formatted_response)
                    print()
                    print("‚ïê" * 80)
                    
                    # Show updated context with nice formatting
                    new_history_count = len(client.get_conversation_history())
                    new_response_id = client.get_last_response_id()
                    
                    print(f"\nÔøΩ Updated State:")
                    print(f"   ‚Ä¢ Conversation Length: {new_history_count} messages")
                    print(f"   ‚Ä¢ Latest Response ID: {new_response_id[:12] if new_response_id else 'None'}...")
                    print(f"   ‚Ä¢ Status: ‚úÖ Success")
                    
                except Exception as e:
                    print("üí• Error occurred:")
                    print("‚îå" + "‚îÄ" * 78 + "‚îê")
                    print(f"‚îÇ ‚ùå {str(e):<74} ‚îÇ")
                    print("‚îî" + "‚îÄ" * 78 + "‚îò")
                    print(f"   ‚Ä¢ Status: ‚ùå Failed")
                    
                    # Check if this was a recoverable error where the agent still succeeded
                    new_history_count = len(client.get_conversation_history())
                    if new_history_count > len(questions) - (len(questions) - i):  # Agent may have succeeded despite error
                        print("   ‚Ä¢ üîÑ Note: Agent may have recovered automatically")
                        print("   ‚Ä¢ üß† Checking if conversation context was still updated...")
                        
                        # Show the latest response if available
                        final_history = client.get_conversation_history()
                        if final_history and len(final_history) >= i:
                            latest_response = final_history[-1]['response'][:100]
                            print(f"   ‚Ä¢ üí° Latest response: {latest_response}...")
                            print("   ‚Ä¢ ‚úÖ Recovery successful - continuing demo")
            
            # Final summary
            print(f"\n" + "=" * 80)
            print("üìã Session Summary")
            print("=" * 80)
            final_history = client.get_conversation_history()
            print(f"‚úÖ Completed {len(questions)} questions")
            print(f"üìö Generated {len(final_history)} conversation exchanges")
            print(f"üß† Final memory state: {len(final_history)} messages in context")
            
            if final_history:
                print(f"üîó Last response ID: {client.get_last_response_id()}")
                print("\nüí° Conversation flow demonstrated:")
                for idx, exchange in enumerate(final_history, 1):
                    print(f"   {idx}. Q: {exchange['question'][:50]}{'...' if len(exchange['question']) > 50 else ''}")
                    print(f"      A: {exchange['response'][:50]}{'...' if len(exchange['response']) > 50 else ''}")
            
            # Option 2: Batch processing (commented out for demo)
            # results = await client.batch_query(questions)
            # for question, response in results.items():
            #     print(f"Q: {question}\nA: {response}\n")
            
    except Exception as e:
        print(f"\nüí• Demo failed with error:")
        print("‚îå" + "‚îÄ" * 78 + "‚îê")
        print(f"‚îÇ ‚ùå {str(e):<74} ‚îÇ")
        print("‚îî" + "‚îÄ" * 78 + "‚îò")
        logger.error(f"Demo failed: {e}")
        raise

def main():
    """Main entry point with enhanced CLI presentation."""
    import sys
    
    # Print banner
    print("\n" + "‚ñà" * 80)
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" + "üöÄ Optimized OpenAI + MCP Integration Demo".center(78) + "‚ñà")
    print("‚ñà" + "Intelligent Jira Assistant with Memory".center(78) + "‚ñà") 
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" * 80)
    
    # Check requirements
    print("\nüîç Pre-flight checks:")
    
    if AGENTS_AVAILABLE:
        print("   ‚úÖ OpenAI Agents library available")
    else:
        print("   ‚ö†Ô∏è  OpenAI Agents library missing - running in mock mode")
        print("   üí° For full functionality, install with: pip install openai-agents")
        print("   üß™ Testing infrastructure will work with mock implementations")
    
    try:
        if DOTENV_AVAILABLE:
            load_dotenv()
        import os
        if os.getenv("OPENAI_API_KEY"):
            print("   ‚úÖ OpenAI API key configured")
        else:
            print("   ‚ö†Ô∏è  OpenAI API key not found in environment")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Environment check failed: {e}")
    
    print("   ‚úÖ Starting demo...")
    
    try:
        asyncio.run(demo_optimized_client())
        
        # Success footer
        print("\n" + "‚ñà" * 80)
        print("‚ñà" + " " * 78 + "‚ñà")
        print("‚ñà" + "‚úÖ Demo completed successfully!".center(78) + "‚ñà")
        print("‚ñà" + "üéâ OpenAI + MCP integration working perfectly".center(78) + "‚ñà")
        print("‚ñà" + " " * 78 + "‚ñà")
        print("‚ñà" * 80)
        print("\nüí° Key features demonstrated:")
        print("   ‚Ä¢ ‚úÖ Seamless OpenAI + MCP integration")
        print("   ‚Ä¢ üß† Conversation memory across queries")
        print("   ‚Ä¢ üîÑ Automatic context management")
        print("   ‚Ä¢ üõ°Ô∏è  Graceful error handling and recovery")
        print("   ‚Ä¢ üìä Real-time status and progress tracking")
        print("   ‚Ä¢ üîß Intelligent error recovery (e.g., link type corrections)")
        print("   ‚Ä¢ üéØ Production-ready resilience patterns")
        
    except KeyboardInterrupt:
        print("\n" + "‚ö†" * 80)
        print("‚ö†" + " " * 78 + "‚ö†")
        print("‚ö†" + "‚èπÔ∏è  Demo interrupted by user (Ctrl+C)".center(78) + "‚ö†")
        print("‚ö†" + " " * 78 + "‚ö†")
        print("‚ö†" * 80)
        sys.exit(0)
        
    except Exception as e:
        print("\n" + "‚ùå" * 80)
        print("‚ùå" + " " * 76 + "‚ùå")
        print("‚ùå" + f"üí• Demo failed: {str(e)[:64]}".center(76) + "‚ùå")
        print("‚ùå" + " " * 76 + "‚ùå")
        print("‚ùå" * 80)
        
        print(f"\nüîç Debug information:")
        print(f"   ‚Ä¢ Error: {type(e).__name__}")
        print(f"   ‚Ä¢ Message: {str(e)}")
        print(f"   ‚Ä¢ Suggestion: Check MCP server connection and OpenAI API key")
        
        logger.error(f"Demo failed with exception: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
